home: 
  home: Home
  welcome: Welcome to my professional portfolio!
  about: |
    I am Martin Majo, **lead developer**, **MLOps Engineer**.

    With almost 6 years of professional experience in software development, I have acquired skills in software development, project management, and artificial intelligence.

    **ðŸ‘ˆ Select my experiences from the menu** to discover my expertise!

  skills:
    primary: "Key Skills :"
    other: "Other Skills :"
    bonus: |
      <p style='font-size:12px'>
      This list is not exhaustive and only reflects the areas where I have the most affinities and expertise.
      </p>

  professional_experience:
    title: "Professional Experience:"
    description: "Description :"

    experiences:
      - company: Peaksys (Cdiscount)
        title: MLOps Engineer
        date: February 2022 - Present ({years} years and {months} months)
        description: |
          Industrialization of the deployment processes for machine learning models.  
          Design, architecture, and development of Python software (libraries, modules).  
          Design and delivery of training sessions for Data Scientists.  
          Debugging and support for Data Science teams.  
          Automation of functional tests.  
          Code optimization.  
        skills: 
          - Python 
          - Docker
          - Kubernetes
          - Azure DevOps
          - Data Science

      - company: JournÃ©e de chasse
        title: Project Lead Developer
        date: March 2020 - September 2021 (1 year and 6 months)
        description: |
          Creation and management of a team (2 developers & 1 UI/UX). Creation of new
          features for the platform. Responsible for servers, platform optimization, and all
          IT aspects of the company (invoices, SEO, AWS, ...). 
        skills: 
          - Angular
          - NodeJS
          - AWS
          - SEO
          - IT
          - Management
          - Python
          - Stripe

      - company: Thales
        title: Software Developer
        date: August 2019 - March 2020 (8 months)
        description: |
          Creation of a platform to manage multiple test benches directly from a browser
          instead of having to stay in the test room.  
          Creation of dynamic reports on all Thales France equipment.
        skills: 
          - Python
          - Birt
          - React

      - company: ShareYourTrip
        title: Full-Stack Developer
        date: February 2018 - March 2019 (1 year and 1 month)
        description: |
          Full Stack Developer for shareyourtrip.fr
        skills: 
          - Symfony
          - Elasticsearch
          - AWS
          - Nginx
          - Python

      - company: Office Toner
        title: Back End Developer
        date: April 2017 - September 2017 (5 months)
        description: |
          Optimization and refactoring of the OfficeToner website
        skills: 
          - Python
          - MongoDB
          - MySQL
          - PHP

      - company: Technicolor
        title: Software Developer
        date: July 2015 - January 2016 (6 months)
        description: |
          Creation of software for automatic sorting of rushes (videos produced during
          film shoots) from clients' external hard drives to the internal server, optimizing
          the time for editors and colorists.
        skills: 
          - Python
          - Qt
          - MySQL

  education:
    title: "Education :"

    educations:
      - school: Epitech
        degree: Master's in Computer Science
        location: Bordeaux, France
        graduated_on: 2019
        link: https://www.epitech.eu/

  certification:
    title: "Certifications :"

    certifications:
      - title: Regression with scikit-learn
        school: DataScientest
        date: 2024
        link: https://files.datascientest.com/certification/0e65da50-fd3f-4000-844c-11ce2f3eeb40.pdf

      - title: Clustering with scikit-learn
        school: DataScientest
        date: 2024
        link: https://files.datascientest.com/certification/45bda0a7-51fc-4687-8df4-b0893ec1420c.pdf

      - title: Classification with scikit-learn
        school: DataScientest
        date: 2024
        link: https://files.datascientest.com/certification/e5e07605-8a1e-4d16-b5c8-e64e3d5f90f7.pdf

      - title: Advanced Classification with scikit-learn
        school: DataScientest
        date: 2024
        link: https://files.datascientest.com/certification/e2de37f4-59bd-45f4-b50f-89444cfe74c3.pdf
  
  navbar:
    download: Download my CV

formations:
  page_title: Training Provided
  title: Training for Cdiscount Data Scientists
  trainer: Trainer
  introduction_text: |
    As an **MLOps Engineer** at Peaksys, I had the opportunity, along with my team, 
    to provide training to data scientists on various key technical topics essential for their daily work. 
    These training sessions aimed to help them better understand the environment in which they operate, 
    improve their technical skills, and adopt best practices in software development.

  sujets_title: Topics Covered
  sujets_text: |
    We covered the following topics during our training sessions:

    - **Git**: Explanation of Git basics, how to version their code, and collaborate effectively.
    - **Docker**: Introduction to Docker, how to containerize their applications for reproducible deployments.
    - **Kubernetes**: Overview of Kubernetes, how to orchestrate their containers at scale.
    - **Azure DevOps**: Using Azure DevOps to manage their projects, CI/CD pipelines, and artifacts.
    - **Monitoring and Logs**: Explanation of the importance of monitoring, how to retrieve and analyze logs to diagnose issues.

  objectifs_title: Training Objectives
  objectifs_text: |
    The training sessions aimed to achieve the following objectives:
    
    - Understand the working environment.
    - Properly deploy their jobs and APIs.
    - Effectively version their code.
    - Adopt best practices in development and documentation.
    - Learn the basics of unit testing and integrate them into a CI/CD pipeline.

  pipeline_title: CI/CD Pipeline Implementation
  pipeline_text: |
    We implemented a CI/CD pipeline for their projects, 
    with minimum requirements for unit test coverage to avoid technical debt.

  support_title: Continuous Support
  support_text: |
    Our team was also present to help them improve their skills on the Python libraries we developed. 
    We were available for support in case of problems, helping them resolve errors and understand error messages.

  conclusion_text: |
    Thanks to these training sessions, Cdiscount's data scientists were able to improve their technical skills and adopt high-quality 
    software development practices, enabling them to work more efficiently and deliver robust, well-documented solutions.

tgi:
  page_title: TGI - Text Generation Inference
  title: Implementation of TGI (Text Generation Inference) with Hugging Face
  introduction_text: |
    At Peaksys, we have implemented an API for text generation using the [**Hugging Face**](https://huggingface.co/docs/text-generation-inference/index) framework. 
    We have also developed a pipeline as code to deploy and manage multiple open source language models (LLMs),
    such as LLaMA3, Mixtral, Mistral, and others, to test and evaluate their performance.
  
  gestion_modeles_title: Model Management
  gestion_modeles_text: |
    Our infrastructure allows deploying multiple LLMs simultaneously, assigning specific resources 
    to each. Each model can be assigned to one or more specific GPUs with a defined amount of resources, 
    ensuring that other models cannot use the allocated resources if they are not available.
    
    - **Supported models**: LLaMA3, Mixtral, Mistral, and many others.
    - **Resource management**: Specific assignment of GPU and resources based on the needs of each model.
  
  ai_gateway_title: AI Gateway
  ai_gateway_text: |
    We have implemented an **AI Gateway** to manage quotas and the usage of each LLM/GPU according to projects,
    jobs, chats, users, etc. This gateway also manages security, authentication, and monitoring.
    This gateway is a single access point allowing access to all our deployed models, and we use Traefik Proxy as the gateway.
    All of this is continuously improving to meet the changing needs of our infrastructure.

  technologies_title: Technologies Used
  technologies_text: |
    To orchestrate and manage our infrastructure, we use the following technologies:
    
    - **Kubernetes (K8s)**: Container orchestration to deploy and manage models at scale.
    - **Helm**: Package manager for Kubernetes, facilitating the deployment and management of applications.
    - **Azure DevOps**: CI/CD platform to automate deployments and manage the application lifecycle.

  conclusion_text: |
    Thanks to this infrastructure, CDiscount and its subsidiaries can use these LLMs in a simple and efficient way, with comprehensive 
    documentation to guide them. This solution offers optimal performance and simplified resource management, 
    allowing our teams to focus on developing and improving their projects while benefiting from well-managed, high-performance language models.

jupyterhub:
  page_title: JupyterHub
  title: Deployment of JupyterHub
  introduction_text: |
    We have deployed [**JupyterHub**](https://jupyter.org/hub) on our Kubernetes cluster to provide a flexible and powerful development environment for our users. 
    This deployment allows our data scientists and developers to work efficiently with resources tailored to their specific needs.

  mise_en_place_title: Setting Up JupyterHub
  mise_en_place_text: |
    The setup of **JupyterHub** on our infrastructure involved several crucial steps:
    
    - **Creating ingress**: Configuring entry points to manage traffic to different JupyterHub instances.
    - **User configuration**: Managing access and permissions for different users.
    - **Adding an authentication plugin for internal OIDC**: Implementing a plugin for secure authentication via our internal OIDC provider.
    - **Deploying a Traefik proxy**: Using Traefik, recommended by the JupyterHub documentation, to manage routes and enhance security.

  creation_images_title: Creating Custom Docker Images
  creation_images_text: |
    We have created custom Docker images for Jupyter servers to meet the diverse needs of our users:
    
    - **Resource selection**: Users can select specific configurations of GPU, CPU, and RAM.
    - **Automatic resource management**: Servers using intensive resources automatically shut down quickly to avoid monopolizing resources.
    - **Python version selection**: Ability to choose different versions of Python for development environments.
    - **Images with GPU drivers**: Availability of images including necessary GPU drivers (CUDA, cuDNN).
    - **Admin page for custom servers**: Creation of an administrative interface allowing the setup of custom servers with additional resources for specific users for a limited duration.
    - **Image optimization**: Reducing the size of Docker images by minimizing unnecessary dependencies.

  gestion_stockage_title: Local Storage Management
  gestion_stockage_text: |
    Each Jupyter server is associated with restricted local storage:
    
    - **Storage limitation**: Each server has a maximum of 20GB local storage on a Persistent Volume Claim (PVC).
    - **Writing in RAM**: Users can write in RAM instead of disk for optimal performance, particularly advantageous for loading models.

  maintenance_title: Continuous Maintenance and Technical Watch
  maintenance_text: |
    We ensure regular updates of Jupyter server images as well as JupyterHub and JupyterLab. 
    With the rapid advancement and increasing use of generative AI, it is crucial to stay up-to-date with the latest technologies and practices.

  conclusion_text: |
    By deploying JupyterHub on our Kubernetes cluster, we have successfully provided a robust and flexible solution 
    for our users, allowing them to benefit from a tailored development environment. 
    With fine-grained resource management and a scalable infrastructure, our teams can focus on 
    innovating and optimizing their projects while benefiting from the latest technological advancements.
    
    Our commitment to maintaining this infrastructure ensures that we remain at the forefront of technology 
    and respond effectively to the changing needs of our users.

mlflow:
  page_title: MLflow
  title: Deployment of MLflow at Peaksys
  introduction_title: Introduction to MLflow
  introduction_text: |
    [**MLflow**](https://mlflow.org/) is an open-source platform designed to manage the entire lifecycle of machine learning models. 
    It includes several key components, including:
    
    - **Model Registry**: Allows managing and versioning machine learning models.
    - **Model Versioning**: Tracks different versions of models for better deployment management.
    - **Model Tracking**: Records parameters, metrics, and artifacts of machine learning experiments.

    At Peaksys, we have deployed MLflow to centralize and optimize the management of our machine learning models.

  deployment_title: Deployment of MLflow
  deployment_text: |
    The deployment of MLflow at Peaksys involved several crucial steps:

    - **Deployment of a reverse proxy**: Using **oauth2_proxy** to manage authentication and security for accessing MLflow.
    - **Creating ingress**: Configuring entry points to direct traffic to the MLflow service.
    - **Creating plugins for MLflow**: Developing custom plugins to enhance MLflow's functionalities.
        - **Authentication**: Plugin to manage user authentication.
        - **Model version management**: Limiting the maximum number of model versions to prevent clutter. Older versions are automatically deleted when the limit is reached, except for certain models with exceptions.
        - **Resource optimization**: Using RAM for temporary storage during model uploads to minimize local storage usage.

  library_title: Python Library for Data Scientists
  library_text: |
    To simplify the use of MLflow by our data scientists, we developed a Python library that extends the MLflow library. 
    This library facilitates the integration of MLflow into their daily workflows, offering additional features and abstractions 
    for more intuitive and efficient use.

  conclusion_text: |
    By implementing MLflow at Peaksys, we have significantly improved our management of machine learning models. 
    The advanced features and custom plugins we developed enable efficient and secure model management, 
    while optimizing resource usage. Our data scientists can now focus on experimentation and innovation, 
    relying on a robust and reliable infrastructure.

argo:
  page_title: Argo Workflows
  title: Deployment of Argo Workflows
  introduction_title: Introduction to Argo Workflows
  introduction_text: |
    [**Argo Workflows**](https://argoproj.github.io/workflows/) is an open-source workflow management tool designed to orchestrate tasks on Kubernetes. 
    It allows defining, scheduling, and managing complex workflows in an efficient and scalable manner. 
    At Peaksys, we have deployed Argo Workflows in production and across all our development environments to optimize job management.

  pipeline_title: "Pipeline as Code"
  pipeline_text: |
    To facilitate the deployment of workflows on Kubernetes, we have implemented a pipeline as code. This pipeline allows for automated and efficient management of deployments.

    - **Pipeline Steps**:
      - **Manual Approval**: A validation step allowing the pipeline to be launched on the selected environment.
      - **Initialization**: Retrieval and formatting of necessary information, including the configuration of jobs to be deployed and the Calico IP for the target Kubernetes environment.
      - **Creation of Workflow Templates**: Generation of WorkflowTemplates manifests for each job to be deployed.
      - **Workflow Deployment**: Creation and updating of workflow templates, followed by deployment on Kubernetes using `argo submit`.

    - **Automation and Security**:
      - **Retrieval of job information**: Cloning of the workflow configuration repository and execution of Python scripts to format the data.
      - **Use of Helm**: Cloning and reformatting of Helm charts to create the necessary configuration templates.
      - **Resource Management**: Allocation of necessary resources and creation of workflow template manifests for each job.
      - **Secure Deployment**: Use of `argo template create` and `argo submit` to ensure secure and automated deployment of workflows.

  orchestration_title: "Efficient Job Orchestration"
  orchestration_text: |
    Argo Workflows enables efficient job orchestration both horizontally and vertically:
    
    - **Horizontal and vertical orchestration**: The ability to split large jobs into smaller tasks allows for more precise resource management and overall better performance.
    - **Optimized resource management**: By balancing the tasks, we can use our resources more efficiently and avoid bottlenecks.

  conclusion_text: |
    By implementing Argo Workflows at Peaksys, we have significantly improved our job management. 
    The automation via a pipeline as code and the optimized resource orchestration allow us to manage our workflows 
    more effectively and robustly, ensuring optimal performance.

tools:
  page_title: Images & Libs Python
  title: Docker Tools and Python Libraries
  docker_header: Python Docker Images
  docker_text: |
    At Peaksys, we have developed and maintain Docker images for various versions of Python to meet the diverse needs of our projects. 
    We manage up to four versions of Python, from the latest release to n-3, while deprecating older versions.
    
    - **Python Versions**: Each version of Python is available in several Debian sizes (slim, alpine, buster, stretch, etc.) depending on specific needs. 
    However, we are increasingly focusing on reducing these variations to simplify management.
    
    - **Docker Image Optimization**: We apply several techniques to optimize our Docker images:
        - **Multi-stage build**: Multi-stage build to reduce image size.
        - **Local storage optimization**: Efficient local storage management to minimize disk space usage.
        - **Environment and volume setup**: Setting up necessary environments and volumes for our Kubernetes cluster.
        - **Optimization for data science libraries**: Specific adjustments for Python data science libraries to improve performance.

  libraries_header: Python Libraries
  libraries_text: |
    We have also created and maintain several Python libraries specific to data science. These libraries enable our data scientists 
    to work more efficiently, quickly, securely, and cleanly. Here are some of our major contributions:
    
    - **Wrappers for existing libraries**: We have developed wrappers for popular libraries like Luigi, MLflow, and Snowflake to simplify their use.
    - **Real-time KPI monitoring library**: Integration with Kafka to monitor performance indicators in real-time.
    - **Security library**: Managing secure machine-to-machine (M2M) or human-to-machine (H2M) calls with our internal OIDC.
    - **Unit testing library**: Tools to facilitate the creation of unit tests, Gherkin, and Wiremock.
    
    These libraries are designed to help our data scientists write more readable, maintainable, and scalable code, allowing them to focus on innovation and achieving ambitious projects.

  conclusion_text: |
    The goal is to provide high-quality tools that optimize team productivity and efficiency. 
    Docker images and Python libraries play a crucial role in the infrastructure, supporting a mission of continuous innovation and delivering cutting-edge data science solutions.
    
    Continuous technical monitoring is carried out to stay at the forefront of new developments and best practices, ensuring that tools and processes evolve with technological advancements.